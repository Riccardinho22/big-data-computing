{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture_15_PageRank.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r8rVH_wrVFN",
        "colab_type": "text"
      },
      "source": [
        "# **Global Constants**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GumGWC6reG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "GDRIVE_DIR = \"/content/gdrive\"\n",
        "GDRIVE_HOME_DIR = GDRIVE_DIR + \"/My Drive\"\n",
        "GDRIVE_DATA_DIR = GDRIVE_HOME_DIR + \"/Teaching/2019-20-BDC/datasets\"\n",
        "DATASET_NODES_URL = \"https://github.com/gtolomei/big-data-computing/raw/master/datasets/web-pages.csv.bz2\"\n",
        "DATASET_LINKS_URL = \"https://github.com/gtolomei/big-data-computing/raw/master/datasets/web-links.csv.bz2\"\n",
        "GDRIVE_DATASET_NODES_FILE = GDRIVE_DATA_DIR + \"/\" + DATASET_NODES_URL.split(\"/\")[-1]\n",
        "GDRIVE_DATASET_LINKS_FILE = GDRIVE_DATA_DIR + \"/\" + DATASET_LINKS_URL.split(\"/\")[-1]\n",
        "\n",
        "RANDOM_SEED = 42 # for reproducibility"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNQWYxnsMqQv",
        "colab_type": "text"
      },
      "source": [
        "# **Spark + Google Colab Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFudfLCRNvXT",
        "colab_type": "text"
      },
      "source": [
        "## **1.** Install PySpark and related dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6vtwpPGKn9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = JAVA_HOME"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOrmY8FiOMwa",
        "colab_type": "text"
      },
      "source": [
        "## **2.** Import useful Python packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh8zPg5APmYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbT2rM-4tvnB",
        "colab_type": "text"
      },
      "source": [
        "## **3.** Create Spark context"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXQOJijlEz3I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\").set('spark.executor.memory', '4G').set('spark.driver.memory', '45G').set('spark.driver.maxResultSize', '10G')\n",
        "\n",
        "# create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC39388R6zwQ",
        "colab_type": "text"
      },
      "source": [
        "##**3.a** Add `GraphFrames` to `SparkContext`\n",
        "\n",
        "[`GraphFrames`](https://graphframes.github.io/graphframes/docs/_site/index.html) is a Python wrapper for [Spark's GraphX API](https://spark.apache.org/graphx/), which unfortunately does not natively support Python at the moment. In order to make `GraphFrames` work, we need to firstly download it and then add it to our current `SparkContext`. The two cells below do exactly that job."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8DwnyKh7Fi3",
        "colab_type": "text"
      },
      "source": [
        "###Download `GraphFrames`\n",
        "`GraphFrames` can be download from a dedicated section of the [Spark packages website](https://spark-packages.org/package/graphframes/graphframes). Please, check out the latest version available, which is also compliant with the Apache Spark version you are working on (i.e., in our case `spark-2.4.x`), for instance: `0.8.0-spark2.4-s_2.11`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHPbltq2Wh16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GRAPHFRAMES_VERSION=\"0.8.0-spark2.4-s_2.11\" # change this string to the correct/latest version if needed\n",
        "GRAPHFRAMES_JAR=\"{:s}.jar\".format(GRAPHFRAMES_VERSION)\n",
        "GRAPHFRAMES_URL=\"http://dl.bintray.com/spark-packages/maven/graphframes/graphframes/{:s}/graphframes-{:s}\".format(GRAPHFRAMES_VERSION, GRAPHFRAMES_JAR)\n",
        "GRAPHFRAMES_DEST=\"/usr/local/lib/python3.6/dist-packages/pyspark/jars/graphframes-{:s}\".format(GRAPHFRAMES_JAR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vA4Wv8MgJBLv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!echo \"!curl -L -o \\\"{GRAPHFRAMES_DEST}\\\" {GRAPHFRAMES_URL}\"\n",
        "!curl -L -o \"{GRAPHFRAMES_DEST}\" {GRAPHFRAMES_URL} #!curl -L -o \"/usr/local/lib/python3.6/dist-packages/pyspark/jars/graphframes-0.8.0-spark2.4-s_2.11.jar\" http://dl.bintray.com/spark-packages/maven/graphframes/graphframes/0.8.0-spark2.4-s_2.11/graphframes-0.8.0-spark2.4-s_2.11.jar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQulD9K6J6Gq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sc.addPyFile(\"/usr/local/lib/python3.6/dist-packages/pyspark/jars/graphframes-{:s}\".format(GRAPHFRAMES_JAR))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewqENIvM7XEx",
        "colab_type": "text"
      },
      "source": [
        "###Import all packages from `graphframes`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efIrd1j6LLDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from graphframes import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV1RghvXuwQG",
        "colab_type": "text"
      },
      "source": [
        "## **4.** Create <code>ngrok</code> tunnel to check the Spark UI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BB4QVvEQuIXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on0pzpMiOA_L",
        "colab_type": "text"
      },
      "source": [
        "## **5.** Link Colab to our Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzUd3JdGS1Tp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Point Colaboratory to our Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(GDRIVE_DIR, force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eRvyl2xwCV6",
        "colab_type": "text"
      },
      "source": [
        "## **6.** Check everything is ok"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fqJ5f0JE3BL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhTN342EEOYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sc._conf.getAll()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSxNCZ0CB87e",
        "colab_type": "text"
      },
      "source": [
        "# **Google Web Graph**\n",
        "\n",
        "In this notebook, we will be using a dataset from the [SNAP Group](https://snap.stanford.edu/data/web-Google.html)*, which was released in 2002 by Google as a part of [Google Programming Contest](http://www.google.com/programming-contest/). \n",
        "\n",
        "More specifically, this dataset contains a snapshot of the **Google Web Graph** consisting of **875,713** nodes (i.e., web pages) connected by **5,105,039** edges (i.e., hyperlinks).\n",
        "\n",
        "The original dataset comes as a single file; in order to facilitate the usage of `GraphFrames` API, it has been split into **2 sources**: \n",
        "-  `web-pages.csv.bz2` containing only the nodes of the graph;\n",
        "-  `web-links.csv.bz2` containing the links between nodes.\n",
        "\n",
        "The reason for that is because `GraphFrames` can create a graph from **2 PySpark DataFrame objects**, representing the set of vertices and edges, respectively. There is a naming convention which those two dataframes must be compliant with: the former should contain at least a column named `id` (indicating the node identifiers), whilst the latter should contain at least two columns named `src` and `dst` to denote the identifiers of the *source* and *destination* nodes connected by a (directed) edge, respectively.\n",
        "\n",
        "For a deeper understanding of the `GraphFrames` package, please refer to the [online user guide](https://graphframes.github.io/graphframes/docs/_site/user-guide.html).\n",
        "\n",
        "The task is to provide movie recommendations to users that they are likely to be interested in and engage with.\n",
        "\n",
        "[**SNAP is the acronym for Stanford Network Analysis Project led by Prof. Jure Leskovec*]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-vLczwIuUhG",
        "colab_type": "text"
      },
      "source": [
        "# **1. Data Collection**\n",
        "\n",
        "This is the first step we need to accomplish before going any further. The dataset will be downloaded directly to our Google Drive, as usual."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5fI3wiGvKBl",
        "colab_type": "text"
      },
      "source": [
        "### **Download dataset file from URL directly to our Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxrLDE_4e7KH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(dataset_url, dest, chunk_size=1024):\n",
        "  response = requests.get(dataset_url, stream=True)\n",
        "  if response.status_code == 200:\n",
        "    with open(dest, \"wb\") as file:\n",
        "      for block in response.iter_content(chunk_size=chunk_size): \n",
        "        if block: \n",
        "          file.write(block)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYgEbYR_Fb4I",
        "colab_type": "text"
      },
      "source": [
        "###**Retrieve the dataset containing nodes (i.e., web pages)** **bold text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quiuGbfyv8vT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Retrieving dataset from URL: {} ...\".format(DATASET_NODES_URL))\n",
        "get_data(DATASET_NODES_URL, GDRIVE_DATASET_NODES_FILE)\n",
        "print(\"Dataset successfully retrieved and stored at: {}\".format(GDRIVE_DATASET_NODES_FILE))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVypRDcNFmST",
        "colab_type": "text"
      },
      "source": [
        "###**Retrieve the dataset containing edges (i.e., hyperlinks)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rqd4l2TAXaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Retrieving dataset from URL: {} ...\".format(DATASET_LINKS_URL))\n",
        "get_data(DATASET_LINKS_URL, GDRIVE_DATASET_LINKS_FILE)\n",
        "print(\"Dataset successfully retrieved and stored at: {}\".format(GDRIVE_DATASET_LINKS_FILE))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DevlrMcPw1ZI",
        "colab_type": "text"
      },
      "source": [
        "### **Read both dataset files (nodes and links) into two Spark Dataframes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKi5Hd60FFcX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nodes_df = spark.read.load(GDRIVE_DATASET_NODES_FILE, \n",
        "                         format=\"csv\", \n",
        "                         sep=\",\", \n",
        "                         inferSchema=\"true\", \n",
        "                         header=\"true\"\n",
        "                         )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouvA62Y3AjyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "links_df = spark.read.load(GDRIVE_DATASET_LINKS_FILE, \n",
        "                         format=\"csv\", \n",
        "                         sep=\",\", \n",
        "                         inferSchema=\"true\", \n",
        "                         header=\"true\"\n",
        "                         )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhRufor_Fx8F",
        "colab_type": "text"
      },
      "source": [
        "#**2. Construct the `GraphFrames` graph object**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z719LqI2AoJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "web_graph = GraphFrame(nodes_df, links_df) # Create the GraphFrame object from the 2 DataFrames"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3APzno3AzZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Take a look at the DataFrames\n",
        "web_graph.vertices.show(5, truncate=False)\n",
        "web_graph.edges.show(5, truncate=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Pj-wqJnA3aF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Check the number of edges of each vertex\n",
        "web_graph.degrees.show(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D06XV2fcNfRi",
        "colab_type": "text"
      },
      "source": [
        "#**3. Execute PageRank on this graph**\n",
        "\n",
        "The `GraphFrames` package has a nice and easy-to-use API for executing PageRank over a graph. There is a method called `pageRank` whose parameters are:\n",
        "-  `resetProbability` which is the complement of the damping factor $d$ (i.e., $1-d$);\n",
        "-  `tol` is the tolerance threshold used to determine the convergence of the PageRank vector;\n",
        "-  `maxIter` specifies the total number of maximum iterations to be run (alternative to `tol`).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4zE-I5WBB4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pr = web_graph.pageRank(resetProbability=0.15, tol=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-5oQqM6BOhK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at the pagerank score for every vertex\n",
        "pr.vertices.show(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9caMP0mGQNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sorting nodes by their value of PageRank (from the highest to the lowest)\n",
        "pr.vertices.sort(['pagerank'], ascending=[0]).show(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AojymK-QK4Vf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scaling PageRank values so that PageRank vector entries sum up to 1\n",
        "pr_sum = pr.vertices.groupBy().sum().collect()[0][0]\n",
        "\n",
        "pr_norm = pr.vertices.withColumn(\"pagerank_norm\", pr.vertices.pagerank/pr_sum)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7no3G3ZTLfJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pr_norm.sort(['pagerank'], ascending=[0]).show(10, truncate=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHVkNdpHTzus",
        "colab_type": "text"
      },
      "source": [
        "#**`GraphFrames` supports many other graph-based algorithms**\n",
        "\n",
        "In addition to PageRank, `GraphFrames` provides support for the following graph-based algorithms:\n",
        "-  Breadth-first search (BFS)\n",
        "-  Connected components\n",
        "-  Strongly connected components\n",
        "-  Label Propagation Algorithm (LPA)\n",
        "-  Shortest paths\n",
        "-  Triangle count\n",
        "\n",
        "For any further information, please refer to the [online user guide](https://graphframes.github.io/graphframes/docs/_site/user-guide.html)."
      ]
    }
  ]
}